# MTH 227 - Applied Natural Language Processing
### Middle East Technical University
---

## Course Overview

Welcome to Applied Natural Language Processing! This course provides a comprehensive introduction to the fundamental concepts and modern techniques in NLP. We will explore how computers can be used to understand, process, and generate human language. The course will have a strong practical focus, with hands-on assignments and a final project where you will build real-world NLP applications. We will cover a range of topics from traditional linguistic models to state-of-the-art deep learning architectures like Transformers and Large Language Models (LLMs).

---

## Instructor Information

* **Instructor:** Batuhan Bardak
* **Email:** bbatuhan@metu.edu.tr
* **Office Hours:** []
* **Teaching Assistant(s):** []

---

## Course Schedule & Logistics

* **Semester:** [e.g., 2025-2026 Fall]
* **Lectures:** []
* **Location:** []
* **Course Website/ODTÜClass:** []

---

## Learning Objectives

Upon successful completion of this course, students will be able to:

* Understand the core challenges and concepts in Natural Language Processing.
* Implement standard NLP pipelines for tasks like text classification, named entity recognition, and sentiment analysis.
* Apply traditional machine learning models and modern deep learning techniques to language data.
* Utilize major NLP libraries and frameworks such as **spaCy**, **NLTK**, **Hugging Face Transformers**, and **PyTorch/TensorFlow**.
* Critically evaluate the performance of NLP models and understand their ethical implications, especially in the context of LLMs.
* Design and execute a complete NLP project from problem formulation to model deployment.

---

## Prerequisites

* Solid programming skills in Python.
* Good understanding of data structures and algorithms.
* Familiarity with probability, statistics, and linear algebra.
* Prior coursework in Machine Learning or Artificial Intelligence is highly recommended.

---

## Weekly Schedule (Tentative)

| Week | Date | Topic | Readings / Notes | Assignment Due |
| :--- | :-------- | :-------------------------------------------------- | :--------------- | :------------- |
| 1 | Sep 29 | Introduction to NLP & Text Processing | Chapter 1, 2 | |
| 2 | Oct 6 | Language Modeling & N-Grams | Chapter 3 | |
| 3 | Oct 13 | Text Classification: Naive Bayes & Logistic Regression | Chapter 4 | HW1 Out |
| 4 | Oct 20 | Vector Semantics & Word Embeddings (Word2Vec, GloVe) | Chapter 6 | |
| 5 | Oct 27 | Neural Networks & Deep Learning Introduction | Chapter 7 | HW1 Due, HW2 Out |
| 6 | Nov 3 | Recurrent Neural Networks (RNNs) & LSTMs | Chapter 9 | |
| 7 | Nov 10 | Sequence-to-Sequence Models & Attention | Chapter 10 | HW2 Due |
| 8 | Nov 17 | **Midterm Exam / Project Proposal Due** | - | Project Proposal |
| 9 | Nov 24 | Transformer Architecture & BERT | Chapter 11 | HW3 Out |
| 10 | Dec 1 | Fine-tuning Pre-trained Language Models (PLMs) | Hugging Face Docs | |
| 11 | Dec 8 | Advanced Prompt Engineering & In-Context Learning | Papers / Blogs | HW3 Due |
| 12 | Dec 15 | Retrieval-Augmented Generation (RAG) & Vector DBs | Papers / Blogs | Project Work |
| 13 | Dec 22 | Responsible AI: Ethics, Bias & Hallucinations in LLMs | Papers | Project Work |
| 14 | Dec 29 | Course Review & Final Project Presentations | - | **Final Project Due** |

---

## Grading

The final grade will be based on the following components:

* **Homework Assignments (2):** 30% (15% each)
* **Final Exam:** 30%
* **Final Project:** 40%
* **Class Participation:** 5%

---

## Resources

#### **Primary Textbook:**

* Jurafsky, D., & Martin, J. H. (2023). *Speech and Language Processing* (3rd ed. draft). [Online Link](https://web.stanford.edu/~jurafsky/slp3/)

#### **Key Python Libraries:**

* [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
* [spaCy](https://spacy.io/)
* [NLTK](https://www.nltk.org/)
* [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/)
* [scikit-learn](https://scikit-learn.org/stable/)

---
# Resources

## Primary Textbook
* Jurafsky, D., & Martin, J. H. (draft 3e). *Speech and Language Processing*. [Online](https://web.stanford.edu/~jurafsky/slp3/)

---

## Key Python Libraries
* **Hugging Face Transformers:** [https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)
* **spaCy:** [https://spacy.io/](https://spacy.io/)
* **NLTK:** [https://www.nltk.org/](https://www.nltk.org/)
* **PyTorch:** [https://pytorch.org/](https://pytorch.org/) or **TensorFlow:** [https://www.tensorflow.org/](https://www.tensorflow.org/)
* **scikit-learn:** [https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)

---

## Additional Learning Resources
A curated list of supplementary materials to deepen your understanding of NLP.

### A. University Courses (full syllabi & videos)
* **Stanford CS224N – NLP with Deep Learning (course page):** [https://web.stanford.edu/class/cs224n/](https://web.stanford.edu/class/cs224n/)
* **Stanford CS224N – Playlist (YouTube):** [https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)
* **CMU 11‑711 / Advanced NLP (Neubig):** [http://phontron.com/class/anlp2023/](http://phontron.com/class/anlp2023/)
* **MIT 6.864 – Advanced NLP (OCW archive):** [https://ocw.mit.edu/courses/6-864-advanced-natural-language-processing-fall-2005/](https://ocw.mit.edu/courses/6-864-advanced-natural-language-processing-fall-2005/)
* **UW CSE 447 – Natural Language Processing:** [https://courses.cs.washington.edu/courses/cse447/](https://courses.cs.washington.edu/courses/cse447/)
* **Stanford CS25 – Transformers United (Playlist):** [https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)

### B. YouTube Channels & Video Lectures (targeted)
* **Andrej Karpathy:** [https://www.youtube.com/@AndrejKarpathy](https://www.youtube.com/@AndrejKarpathy)
    * “makemore”: [https://www.youtube.com/watch?v=PaCmpygFfXo](https://www.youtube.com/watch?v=PaCmpygFfXo)
    * “Let’s build GPT from scratch”: [https://www.youtube.com/watch?v=kCc8FmEb1nY](https://www.youtube.com/watch?v=kCc8FmEb1nY)
* **Hugging Face:** [https://www.youtube.com/@HuggingFace](https://www.youtube.com/@HuggingFace)
* **Chris McCormick AI:** [https://www.youtube.com/c/ChrisMcCormickAI](https://www.youtube.com/c/ChrisMcCormickAI)
* **CodeEmporium:** [https://www.youtube.com/c/CodeEmporium](https://www.youtube.com/c/CodeEmporium)

#### C. Blogs & Tutorials
* [The Illustrated Transformer by Jay Alammar](http://jalammar.github.io/illustrated-transformer/) - A fantastic visual explanation of the Transformer architecture.
* [Distill.pub](https://distill.pub/) - In-depth, interactive articles on machine learning concepts.
* [Sebastian Raschka's Blog](https://sebastianraschka.com/blog/index.html) - Clear and detailed articles on ML and deep learning.
 
### D. Seminal & Highly Useful Papers (by topic)
* **Transformers & Attention:**
    * Attention Is All You Need — [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
    * The Annotated Transformer — [http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
* **BERT & Encoders:**
    * BERT — [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
    * RoBERTa — [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)
    * XLNet — [https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)
    * ELECTRA — [https://arxiv.org/abs/2003.10555](https://arxiv.org/abs/2003.10555)
    * T5 — [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)
* **LLMs & Scaling:**
    * GPT‑3 — [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
    * Chinchilla — [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)
    * Meta Llama 3 — [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)
    * Mistral 7B — [https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825)
* **Tokenization:**
    * BPE for NMT — [https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)
    * SentencePiece — [https://arxiv.org/abs/1808.06226](https://arxiv.org/abs/1808.06226)
* **Sentence‑level Embeddings:**
    * Sentence‑BERT — [https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084)
* **Responsible AI:**
    * Model Cards — [https://arxiv.org/abs/1810.03993](https://arxiv.org/abs/1810.03993)
    * Datasheets for Datasets — [https://arxiv.org/abs/1803.09010](https://arxiv.org/abs/1803.09010)
    * “Stochastic Parrots” — [https://dl.acm.org/doi/10.1145/3442188.3445922](https://dl.acm.org/doi/10.1145/3442188.3445922)

### E. Practical Docs & Tutorials (use in assignments)
* **Transformers (HF) Docs:** [https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)
* **Hugging Face – Free NLP Course:** [https://huggingface.co/learn/nlp-course/](https://huggingface.co/learn/nlp-course/)
* **spaCy Docs:** [https://spacy.io/usage](https://spacy.io/usage) & **spaCy Course:** [https://course.spacy.io/](https://course.spacy.io/)
* **Hugging Face Datasets (Quickstart):** [https://huggingface.co/docs/datasets/quickstart](https://huggingface.co/docs/datasets/quickstart)
* **Prompting:**
    * OpenAI Best Practices — [https://platform.openai.com/docs/guides/prompt-engineering](https://platform.openai.com/docs/guides/prompt-engineering)
    * DAIR.AI Prompting Guide — [https://www.promptingguide.ai/](https://www.promptingguide.ai/)
* **Parameter‑Efficient FT:**
    * LoRA — [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)
    * QLoRA — [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)
    * PEFT Library — [https://huggingface.co/docs/peft/index](https://huggingface.co/docs/peft/index)
    * Llama Recipes — [https://github.com/meta-llama/llama-recipes](https://github.com/meta-llama/llama-recipes)

### F. Vector Databases & RAG Stack
* **Embedding & Retrieval**
    * Sentence‑Transformers (SBERT) — [https://www.sbert.net/](https://www.sbert.net/)
    * FAISS — [https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss)
    * Weaviate — [https://weaviate.io/developers/weaviate](https://weaviate.io/developers/weaviate)
    * Qdrant — [https://qdrant.tech/documentation/](https://qdrant.tech/documentation/)
    * Chroma — [https://docs.trychroma.com/](https://docs.trychroma.com/)
    * pgvector — [https://github.com/pgvector/pgvector](https://github.com/pgvector/pgvector)
* **RAG Frameworks & Evaluation**
    * LangChain (RAG) — [https://python.langchain.com/docs/use_cases/rag/](https://python.langchain.com/docs/use_cases/rag/)
    * LlamaIndex — [https://docs.llamaindex.ai/](https://docs.llamaindex.ai/)
    * Haystack — [https://docs.haystack.deepset.ai/docs](https://docs.haystack.deepset.ai/docs) / [https://haystack.deepset.ai/tutorials](https://haystack.deepset.ai/tutorials)
    * RAGAS — [https://docs.ragas.io/](https://docs.ragas.io/) / [https://github.com/explodinggradients/ragas](https://github.com/explodinggradients/ragas)

### G. Curated GitHub Lists (fast discovery of tools, papers, repos)
* **awesome‑nlp** — [https://github.com/keon/awesome-nlp](https://github.com/keon/awesome-nlp)
* **Awesome‑LLM** — [https://github.com/Hannibal046/Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)
* **awesome‑huggingface** — [https://github.com/huggingface/awesome-huggingface](https://github.com/huggingface/awesome-huggingface)
* **awesome‑rag** — [https://github.com/likejazz/awesome-rag](https://github.com/likejazz/awesome-rag)


---


## Academic Integrity

All work submitted in this course must be your own. Any form of academic dishonesty, including plagiarism and cheating, will be taken very seriously and will result in a failing grade for the course, in accordance with METU's policies on academic integrity. When in doubt, please ask the instructor or TAs for clarification.

---

## How to Use This Repository

This repository will contain lecture notes, code examples, assignment specifications, and other resources. Please clone it to your local machine to stay updated.

```bash
git clone [URL_to_this_repository]
